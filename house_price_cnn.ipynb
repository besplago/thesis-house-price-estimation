{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class House: \n",
    "  def __init__(self, address, postal_code, type, real_price, \n",
    "                size, basement_size, rooms, year_built, \n",
    "                year_rebuilt, energy_label, image_floorplan): \n",
    "    \n",
    "    #Textual Data \n",
    "    self.address = address  \n",
    "    self.postal_code = postal_code\n",
    "    self.type = type\n",
    "    self.price = real_price\n",
    "    self.size = size\n",
    "    self.basement_size = basement_size\n",
    "    self.rooms = rooms\n",
    "    self.year_built = year_built\n",
    "    self.year_rebuilt = year_rebuilt\n",
    "    self.energy_label = energy_label\n",
    "\n",
    "    #Image Data \n",
    "    self.image_floorplan = image_floorplan\n",
    "    \n",
    "    #Predictions \n",
    "    self.predicted_price = None\n",
    "\n",
    "def load_jpg_and_json(folder_path:str) -> (dict, cv2):\n",
    "  files = os.listdir(folder_path)\n",
    "  jpg_file = None\n",
    "  json_file = None\n",
    "  # Find the jpg and json file in the folder\n",
    "  for filename in files:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "      jpg_file = os.path.join(folder_path, filename)\n",
    "    elif filename.endswith(\".json\"):\n",
    "      json_file = os.path.join(folder_path, filename)\n",
    "  \n",
    "  # Load the jpg and json file\n",
    "  if jpg_file:\n",
    "    image_data = cv2.imread(jpg_file)  \n",
    "  else:\n",
    "    image_data = None\n",
    "  if json_file:\n",
    "    with open(json_file, \"r\") as f:\n",
    "      json_data = json.load(f) \n",
    "  return json_data, image_data\n",
    "\n",
    "def create_house_instance(json_data, jpg): \n",
    "  address = json_data[\"address\"]\n",
    "  postal_code = json_data[\"postal_code\"]\n",
    "  type = json_data[\"type\"]\n",
    "  price = json_data[\"price\"]\n",
    "  size = json_data[\"size\"]\n",
    "  basement_size = json_data[\"basement_size\"]\n",
    "  rooms = json_data[\"rooms\"]\n",
    "  year_built = json_data[\"year_built\"]\n",
    "  year_rebuilt = json_data[\"year_rebuilt\"] if \"year_rebuilt\" in json_data else json_data[\"year_built\"]\n",
    "  energy_label = json_data[\"energy_label\"]\n",
    "  image_floorplan = jpg\n",
    "\n",
    "  house = House(address, postal_code, type, price, \n",
    "                size, basement_size, rooms, year_built, \n",
    "                year_rebuilt, energy_label, image_floorplan)\n",
    "  return house\n",
    "\n",
    "def load_houses(folder_path:str):\n",
    "  houses = []\n",
    "  for folder in os.listdir(folder_path):\n",
    "    json_data, jpg = load_jpg_and_json(os.path.join(folder_path, folder))\n",
    "    house = create_house_instance(json_data, jpg)\n",
    "    houses.append(house)\n",
    "  return houses\n",
    "\n",
    "#If we want to work with a DF \n",
    "def data_to_DF(houses: list[House])-> pd.DataFrame:\n",
    "  data = []\n",
    "  for house in houses:\n",
    "    data.append([house.address, house.postal_code, house.type, house.price, \n",
    "                house.size, house.basement_size, house.rooms, house.year_built, \n",
    "                house.year_rebuilt, house.energy_label, house.image_floorplan])\n",
    "    \n",
    "  df = pd.DataFrame(data, columns = [\"address\", \"postal_code\", \"type\", \"price\", \n",
    "                \"size\", \"basement_size\", \"rooms\", \"year_built\", \n",
    "                \"year_rebuilt\", \"energy_label\", \"image_floorplan\"])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../nybolig-scrape/output\"\n",
    "houses = load_houses(path)\n",
    "houses_df = data_to_DF(houses)\n",
    "postal_codes = (1000, 2900)\n",
    "types = [\"Villa\", \"RÃ¦kkehus\", \"Ejerlejlighed\"]\n",
    "data = houses_df[(houses_df['postal_code'] >= postal_codes[0]) & (houses_df['postal_code'] <= postal_codes[1])]\n",
    "display(houses_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add labels-column to the data-points, based on prices\n",
    "Creates labels based on a normal distribution around the data.\n",
    "That is, we have more labels the closer we are to the mean price, and less the further away we are.\n",
    "Return a data-Frame with the labels and the label codes. \n",
    "\"\"\"\n",
    "def label_data(data: pd.DataFrame, num_labels:int)-> pd.DataFrame:\n",
    "  #We want to predict the price of the house \n",
    "  min = data['price'].min()\n",
    "  first_quan = data['price'].quantile(0.25)\n",
    "  mean = data['price'].mean()\n",
    "  third_quan = data['price'].quantile(0.75)\n",
    "  max = data['price'].max()\n",
    "  #Create a normal distribution of the labels\n",
    "  f1 = np.linspace(0, min, round(num_labels*0.023))\n",
    "  f2 = np.linspace(min, first_quan, round(num_labels*0.14))\n",
    "  f3 = np.linspace(first_quan, mean, round(num_labels*0.34))\n",
    "  f4 = np.linspace(mean, third_quan, round(num_labels*0.34))\n",
    "  f5 = np.linspace(third_quan,max, round(num_labels*0.14))\n",
    "  f6 = np.linspace(max, max*2, round(num_labels*0.023))\n",
    "  potential_labels = np.concatenate((f1, f2, f3, f4, f5, f6))\n",
    "\n",
    "  #Create the label codes\n",
    "  label_codes = [(i, label) for i, label in enumerate(potential_labels)]\n",
    "  \n",
    "  #Create the labels\n",
    "  price_labels = []\n",
    "  price_bracket = []\n",
    "  for price in data['price']:\n",
    "    diff = abs(potential_labels - price)\n",
    "    index = np.argmin(diff)\n",
    "    price_labels.append(index)\n",
    "    left = potential_labels[index-1] if index > 0 else potential_labels[index]\n",
    "    right = potential_labels[index+1] if index < len(potential_labels)-1 else potential_labels[index]\n",
    "    price_bracket.append((left, right))\n",
    "\n",
    "  data['label'] = price_labels\n",
    "  data['price_bracket'] = price_bracket\n",
    "  return data, label_codes\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "  gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  gray_scale = gray_scale.reshape(gray_scale.shape[0], gray_scale.shape[1], 1)\n",
    "  return gray_scale\n",
    "\n",
    "def resize_image(image, target_width, target_height):\n",
    "  return cv2.resize(image, (target_width, target_height), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def normalize_image(image):\n",
    "  return image/255\n",
    "\n",
    "def rescale_plan_by_size(image, size, basement_size): \n",
    "  canvas_size = image.shape[0]\n",
    "  size_ratio = size/canvas_size\n",
    "  basement_size_ratio = basement_size/canvas_size\n",
    "  #Downsize the image, but make it fit the canvas size\n",
    "  image = resize_image(image, int(image.shape[1]*size_ratio), int(image.shape[0]*size_ratio))\n",
    "  \n",
    "  \n",
    "\n",
    "def preprocces_data(data: pd.DataFrame)-> pd.DataFrame:\n",
    "  data = data.drop(columns=[\"address\"])\n",
    "  data['basement_size'] = data[\"basement_size\"].fillna(0)\n",
    "  data['year_rebuilt'] = data['year_rebuilt'].where(~data['year_rebuilt'].isna(), data['year_built']).astype(int)\n",
    "  data['type'] = data['type'].astype('category').cat.codes\n",
    "  data['energy_label'] = data['energy_label'].astype('category').cat.codes\n",
    "  data.dropna(inplace=True)\n",
    "  \n",
    "  #data['image_floorplan'] = data['image_floorplan'].apply(convert_to_grayscale)\n",
    "  \n",
    "  #data['image_floorplan'] = data['image_floorplan'].apply(scale_image)\n",
    "  \n",
    "  #Optimal: use ImageGenerator to augment the images\n",
    "  return data\n",
    "\n",
    "preprocessed_data = preprocces_data(houses_df)\n",
    "display(preprocessed_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_data['image_floorplan'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(preprocessed_data['image_floorplan'].values, preprocessed_data['price'], test_size = 0.2, random_state = 0)\n",
    "# Split the data into train, validation and test sets with a 60-20-20 ratio\n",
    "train_df, test_df = train_test_split(preprocessed_data, test_size=0.2, random_state=0)\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.20, random_state=0)\n",
    "\n",
    "# Reshape the images to the desired size\n",
    "target_height, target_width = 224*2, 224*2\n",
    "train_images = np.array([cv2.resize(image, (target_height, target_width)) for image in train_df[\"image_floorplan\"]])\n",
    "valid_images = np.array([cv2.resize(image, (target_height, target_width)) for image in valid_df[\"image_floorplan\"]])\n",
    "test_images = np.array([cv2.resize(image, (target_height, target_width)) for image in test_df[\"image_floorplan\"]])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(zip(train_images[0:9], train_df[\"price\"][0:9])):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(f\"{label}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "print(train_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.plainenglish.io/judge-the-book-price-by-its-cover-with-image-regression-using-cnns-python-770707e4fe67\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPool2D, MaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (train_images[0].shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n",
    "                 input_shape = input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "#model.add(Dense(n_classes, activation='softmax'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "learning_rate = 0.01\n",
    "\n",
    "model.compile(loss = 'mae',\n",
    "              optimizer = Adam(learning_rate))\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "model.fit(train_images, train_df['price'], validation_data=(valid_images, valid_df['price']), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model \n",
    "model.evaluate(test_images, test_df['price'])\n",
    "\n",
    "#Make predictions\n",
    "predictions = model.predict(test_images).flatten()\n",
    "\n",
    "print(predictions[:20])\n",
    "print(test_df['price'][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_results(model_name, y_test, y_pred):\n",
    "    # Plotting the test set results\n",
    "    plt.scatter(y_test, y_pred)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = y_pred - y_test\n",
    "\n",
    "    # Calculate distances from the perfect fit line\n",
    "    distances = np.abs(y_test - y_pred)\n",
    "\n",
    "    # Define color gradient based on distances\n",
    "    colors = distances / np.max(distances)  # Normalize distances to range [0, 1]\n",
    "    # colors = plt.cm.RdYlGn_r(colors)  # Reverse the colormap: green (furthest), red (closest)\n",
    "\n",
    "    # Plot true values vs predictions with color gradient\n",
    "    plt.scatter(y_test, y_pred, c=colors)\n",
    "    plt.xlabel('True values')\n",
    "    plt.ylabel('Predictions')\n",
    "    # Plot the perfect fit line\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], c='r')\n",
    "    # Name the perfect fit line\n",
    "    plt.title(f'True values vs Predictions ({model_name})')\n",
    "    plt.colorbar(label='Distance from Diagonal')\n",
    "    plt.legend(['Test values', 'Perfect fit'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot residuals\n",
    "    plt.scatter(y_pred, residuals, c=colors)\n",
    "    plt.hlines(y=0, xmin=y_test.min(), xmax=y_test.max(), colors='r')\n",
    "    plt.title(f'Residual plot ({model_name})')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.colorbar(label='Distance from Diagonal')\n",
    "    plt.legend(['Residuals', 'Perfect fit'])\n",
    "    plt.show()\n",
    "\n",
    "plot_regression_results('CNN', test_df['price'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
